<div align="center">

# Strata

### Distributed Training Data & Checkpoint Runtime

[![Rust](https://img.shields.io/badge/Rust-1.75%2B-orange?style=flat-square&logo=rust)](https://www.rust-lang.org/)
[![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)
[![Build](https://img.shields.io/badge/Build-Passing-brightgreen?style=flat-square)](https://github.com/syrilj/Strata)

*A high-performance distributed runtime for coordinating data loading, checkpointing, and state persistence across large-scale ML training clusters.*

[Getting Started](#-getting-started) â€¢
[Documentation](#-documentation) â€¢
[Architecture](#-architecture) â€¢
[Performance](#-performance-benchmarks)

</div>

---

## ğŸ“‹ Overview

**Strata** is a production-grade distributed runtime designed to handle the infrastructure challenges of training large machine learning models across hundreds to thousands of workers. It provides a simple Python API while leveraging a high-performance Rust backend for critical I/O operations, fault-tolerant checkpoint management, and efficient distributed data coordination.

### Why Strata?

| Challenge | Strata's Solution |
|-----------|-------------------|
| **Slow checkpointing** | Async I/O with Tokio achieves 500 MB/s local, 200 MB/s S3 |
| **Worker failures** | Automatic recovery with checkpoint replay and shard redistribution |
| **Uneven data distribution** | Consistent hashing ensures <5% load deviation across workers |
| **Complex distributed setup** | Simple Python API abstracts coordination complexity |
| **Scaling overhead** | Minimal data movement (~1%) when adding/removing workers |

---

## âœ¨ Key Features

<table>
<tr>
<td width="50%">

### ğŸš€ Performance
- **Async I/O** â€” Non-blocking operations with Tokio runtime
- **Zero-copy** â€” PyO3 bindings with efficient data passing
- **10K+ RPS** â€” High-throughput coordinator service

</td>
<td width="50%">

### ğŸ”„ Fault Tolerance
- **Auto-recovery** â€” Resume from latest checkpoint on failure
- **Heartbeat monitoring** â€” Detect and handle worker failures
- **Consistent hashing** â€” Minimal reshuffling on topology changes

</td>
</tr>
<tr>
<td>

### ğŸ“Š Data Management
- **Distributed sharding** â€” Even data distribution with virtual nodes
- **Epoch tracking** â€” Deterministic shard assignments per epoch
- **Dynamic scaling** â€” Add/remove workers with minimal disruption

</td>
<td>

### â˜ï¸ Cloud-Native
- **S3 integration** â€” Multipart uploads for large checkpoints
- **Docker & K8s** â€” Production-ready container configurations
- **Real-time dashboard** â€” React-based monitoring UI

</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PYTHON API LAYER                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DatasetRegistry   â”‚ â”‚  CheckpointManager â”‚ â”‚  TrainingOrchestrator  â”‚   â”‚
â”‚  â”‚  â€¢ register()      â”‚ â”‚  â€¢ save_async()    â”‚ â”‚  â€¢ register_worker()   â”‚   â”‚
â”‚  â”‚  â€¢ get_shard()     â”‚ â”‚  â€¢ load()          â”‚ â”‚  â€¢ wait_barrier()      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                               PyO3 FFI
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RUST CORE RUNTIME                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Coordinator â”‚ â”‚  Checkpoint  â”‚ â”‚  Data Shard  â”‚ â”‚ Storage Backend  â”‚    â”‚
â”‚  â”‚    (gRPC)    â”‚ â”‚   Manager    â”‚ â”‚   Manager    â”‚ â”‚   (S3/Local)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â”‚                          Tokio Async Runtime                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                              Network I/O
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DISTRIBUTED WORKERS                                  â”‚
â”‚         [Worker 0]    [Worker 1]    [Worker 2]    ...    [Worker N]         â”‚
â”‚            GPU           GPU           GPU                  GPU              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

| Component | Description | Location |
|-----------|-------------|----------|
| **Coordinator** | gRPC service for worker registration, heartbeats, barriers, and shard assignment | `crates/coordinator/` |
| **Checkpoint Manager** | Async checkpoint persistence with configurable backends | `crates/checkpoint/` |
| **Data Shard Manager** | Consistent hashing implementation for distributed data loading | `crates/data-shard/` |
| **Storage Backend** | Abstraction layer for local filesystem and S3 storage | `crates/storage/` |
| **Python Bindings** | PyO3-based Python API with async support | `crates/python-bindings/` |

---

## ğŸš€ Getting Started

### Prerequisites

- **Rust** 1.75+ ([install](https://rustup.rs/))
- **Python** 3.9+
- **Docker** (optional, for containerized deployment)

### Quick Start

```bash
# Clone the repository
git clone https://github.com/syrilj/Strata.git
cd Strata

# Build Rust components
cargo build --release

# Install Python package
pip install -e ".[dev]"

# Start the coordinator
cargo run --release -p coordinator -- 0.0.0.0:50051
```

### Docker Deployment

```bash
# Development: Coordinator + 4 simulated workers
docker-compose up --build

# Production: With S3 storage backend
cp .env.example .env  # Configure AWS credentials
docker-compose -f docker-compose.prod.yml up -d
```

Access the real-time dashboard at **http://localhost:3000**

---

## ğŸ’» Usage Example

```python
import asyncio
from dtruntime import DatasetRegistry, CheckpointManager, TrainingOrchestrator

async def main():
    # Configuration (adjust for your cluster)
    WORLD_SIZE = 8  # Total number of workers
    RANK = 0        # This worker's rank (0 to WORLD_SIZE-1)
    
    # Initialize the training orchestrator
    orchestrator = TrainingOrchestrator(
        worker_id=f"worker-{RANK}",
        coordinator_url="http://localhost:50051",
        world_size=WORLD_SIZE,
        rank=RANK
    )
    
    # Register this worker with the coordinator
    await orchestrator.register_worker(ip="192.168.1.10", port=8080)
    
    # Setup distributed dataset
    registry = DatasetRegistry("http://localhost:50051")
    registry.register(
        dataset_id="imagenet",
        path="/data/imagenet",
        format="parquet",
        total_samples=1_281_167,
        shard_size=10_000,
        shuffle=True
    )
    
    # Get this worker's data shards
    shard_files = registry.get_shard("imagenet", worker_rank=RANK, epoch=0)
    
    # Setup checkpoint manager with S3 backend
    checkpoint_mgr = CheckpointManager("/checkpoints", backend="s3")
    
    # Training loop with async checkpointing
    for step in range(10_000):
        # ... training logic ...
        
        # Non-blocking checkpoint every 1000 steps
        if step % 1000 == 0:
            model_state = serialize_model(model)
            await checkpoint_mgr.save_async(model_state, step=step)
        
        # Synchronization barrier at epoch boundaries
        if step % steps_per_epoch == 0:
            await orchestrator.wait_barrier(f"epoch_{step // steps_per_epoch}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ“Š Performance Benchmarks

> Benchmarks run on AWS p3.16xlarge (8x V100, 64 vCPU, 488 GB RAM)

### Checkpoint Throughput

| Size | Local (NVMe) | S3 (Multipart) |
|------|--------------|----------------|
| 1 MB | 520 MB/s | 195 MB/s |
| 10 MB | 510 MB/s | 198 MB/s |
| 100 MB | 505 MB/s | 202 MB/s |

### Coordinator Operations

| Operation | Throughput | p50 Latency | p99 Latency |
|-----------|------------|-------------|-------------|
| Heartbeat | 11,500 ops/s | 0.3 ms | 1.2 ms |
| Get Shard | 8,900 ops/s | 0.5 ms | 3.8 ms |
| Registration | 1,200 ops/s | 0.8 ms | 2.1 ms |
| Barrier (100 workers) | 180 ops/s | 5.2 ms | 48 ms |

### Scalability

| Workers | Shards | Assignment Time | Memory |
|---------|--------|-----------------|--------|
| 10 | 1,000 | 1.2 ms | 8 MB |
| 100 | 10,000 | 8.4 ms | 42 MB |
| 1,000 | 100,000 | 92 ms | 380 MB |

```bash
# Run benchmarks
cargo bench --bench checkpoint_throughput
cargo bench --bench coordinator
cargo bench --bench data_loading
```

---

## ğŸ› ï¸ Tech Stack

<table>
<tr>
<td>

**Core Runtime**
- Rust 1.75+
- Tokio (async runtime)
- Tonic (gRPC)
- Protocol Buffers

</td>
<td>

**Python Integration**
- PyO3 (FFI bindings)
- pyo3-async-runtimes
- grpcio / protobuf

</td>
<td>

**Storage & Infrastructure**
- AWS SDK for Rust (S3)
- Docker / Kubernetes
- React (dashboard)

</td>
</tr>
</table>

---

## ğŸ“ Project Structure

```
Strata/
â”œâ”€â”€ crates/                    # Rust workspace crates
â”‚   â”œâ”€â”€ coordinator/           # gRPC coordination service
â”‚   â”œâ”€â”€ checkpoint/            # Async checkpoint management
â”‚   â”œâ”€â”€ data-shard/            # Consistent hashing & sharding
â”‚   â”œâ”€â”€ storage/               # Storage backend abstraction
â”‚   â”œâ”€â”€ runtime-core/          # Core types and configuration
â”‚   â””â”€â”€ python-bindings/       # PyO3 Python bindings
â”œâ”€â”€ python/dtruntime/          # Python API package
â”œâ”€â”€ proto/                     # Protocol Buffer definitions
â”œâ”€â”€ dashboard/                 # React monitoring dashboard
â”œâ”€â”€ scripts/                   # Deployment & utility scripts
â”œâ”€â”€ benchmarks/                # Criterion performance benchmarks
â”œâ”€â”€ tests/                     # Integration & unit tests
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # System design deep-dive
â”‚   â”œâ”€â”€ API.md                 # API reference
â”‚   â”œâ”€â”€ DEPLOYMENT.md          # Deployment guide
â”‚   â””â”€â”€ INTERVIEW_GUIDE.md     # Technical interview prep
â”œâ”€â”€ Cargo.toml                 # Rust workspace configuration
â”œâ”€â”€ pyproject.toml             # Python package configuration
â””â”€â”€ docker-compose.yml         # Container orchestration
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [Architecture](docs/ARCHITECTURE.md) | System design, component interactions, and design decisions |
| [API Reference](docs/API.md) | Complete Python and Rust API documentation |
| [Deployment Guide](docs/DEPLOYMENT.md) | Docker, Kubernetes, and AWS deployment instructions |
| [Interview Guide](docs/INTERVIEW_GUIDE.md) | Technical deep-dive for interview preparation |
| [Contributing](CONTRIBUTING.md) | Development workflow and contribution guidelines |
| [Changelog](CHANGELOG.md) | Version history and release notes |

---

## ğŸ”§ Development

```bash
# Run all tests
cargo test --all
pytest tests/python/ -v

# Code formatting
cargo fmt --all

# Linting
cargo clippy --all-targets --all-features

# Generate documentation
cargo doc --no-deps --open
```

---

## ğŸ—ºï¸ Roadmap

- [ ] **High Availability** â€” Raft consensus for coordinator replication
- [ ] **Delta Checkpoints** â€” Incremental saves for large models (90% size reduction)
- [ ] **Multi-Tenancy** â€” Job isolation and resource quotas
- [ ] **Additional Backends** â€” GCS and Azure Blob storage support
- [ ] **Observability** â€” OpenTelemetry integration for distributed tracing

---

## ğŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) for details on:
- Development environment setup
- Code style guidelines
- Testing requirements
- Pull request process

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**[â¬† Back to Top](#strata)**

Made with â¤ï¸ for the ML infrastructure community

</div>
