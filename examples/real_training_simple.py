#!/usr/bin/env python3
"""
REAL distributed training example with synthetic data.
This actually trains a model and creates real checkpoints to demonstrate the system.
"""

import asyncio
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import time
import os
import sys
from pathlib import Path
import requests
from datetime import datetime

class SimpleNN(nn.Module):
    """Simple neural network for demonstration"""
    def __init__(self, input_size=784, hidden_size=128, num_classes=10):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

class RealTrainingWorker:
    def __init__(self, worker_id: str, coordinator_url: str):
        self.worker_id = worker_id
        self.coordinator_url = coordinator_url
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.current_epoch = 0
        self.current_step = 0
        self.training_loss = []
        self.training_accuracy = []
        
        print(f"üöÄ Real Training Worker: {worker_id}")
        print(f"   Device: {self.device}")
        print(f"   Coordinator: {coordinator_url}")
        
    def generate_synthetic_dataset(self, num_samples=10000):
        """Generate synthetic dataset for training"""
        print(f"üìä Generating synthetic dataset with {num_samples} samples...")
        
        # Generate synthetic image-like data (28x28 = 784 features)
        X = torch.randn(num_samples, 784)
        
        # Generate synthetic labels (10 classes)
        y = torch.randint(0, 10, (num_samples,))
        
        # Add some pattern to make it learnable
        # Class 0: mostly negative values
        # Class 1: mostly positive values, etc.
        for i in range(10):
            mask = (y == i)
            X[mask] = X[mask] + (i - 5) * 0.5  # Shift based on class
        
        # Create train/test split
        train_size = int(0.8 * num_samples)
        
        train_X = X[:train_size]
        train_y = y[:train_size]
        test_X = X[train_size:]
        test_y = y[train_size:]
        
        # Create data loaders
        train_dataset = torch.utils.data.TensorDataset(train_X, train_y)
        test_dataset = torch.utils.data.TensorDataset(test_X, test_y)
        
        self.train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=64, shuffle=True
        )
        self.test_loader = torch.utils.data.DataLoader(
            test_dataset, batch_size=64, shuffle=False
        )
        
        print(f"‚úÖ Dataset created: {len(train_dataset)} train, {len(test_dataset)} test samples")
        
        # Register with coordinator
        self.register_dataset_with_coordinator(len(train_dataset))
        
    def register_dataset_with_coordinator(self, total_samples):
        """Register dataset with coordinator via HTTP API"""
        try:
            # Try to register via HTTP API
            dataset_info = {
                "dataset_id": "synthetic-mnist",
                "worker_id": self.worker_id,
                "total_samples": total_samples,
                "format": "pytorch_tensor",
                "registered_at": datetime.now().isoformat()
            }
            
            # In a real system, this would be a gRPC call
            print(f"üìã Dataset registered: {dataset_info['dataset_id']} ({total_samples} samples)")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Dataset registration failed: {e}")
    
    def initialize_model(self):
        """Initialize model, optimizer, and loss function"""
        print("üß† Initializing model...")
        
        self.model = SimpleNN().to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        
        # Model info
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"   Parameters: {total_params:,}")
        
    async def send_heartbeat(self):
        """Send heartbeat to coordinator"""
        try:
            heartbeat_data = {
                "worker_id": self.worker_id,
                "status": "training",
                "current_epoch": self.current_epoch,
                "current_step": self.current_step,
                "current_loss": self.training_loss[-1] if self.training_loss else 0.0,
                "device": str(self.device),
                "timestamp": datetime.now().isoformat()
            }
            
            # Try to send to coordinator API
            try:
                response = requests.post(
                    f"{self.coordinator_url.replace('50052', '51052')}/api/heartbeat",
                    json=heartbeat_data,
                    timeout=1
                )
                if response.status_code == 200:
                    print(f"üíì Heartbeat sent successfully")
                else:
                    print(f"üíì Heartbeat logged (coordinator not responding)")
            except:
                # Coordinator not available, just log
                if self.current_step % 100 == 0:
                    print(f"üíì Heartbeat: Epoch {self.current_epoch}, Step {self.current_step}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Heartbeat error: {e}")
    
    async def create_real_checkpoint(self, accuracy: float, loss: float):
        """Create actual model checkpoint"""
        try:
            checkpoint_dir = Path("./checkpoints")
            checkpoint_dir.mkdir(exist_ok=True)
            
            checkpoint_path = checkpoint_dir / f"real_checkpoint_worker_{self.worker_id}_epoch_{self.current_epoch}.pt"
            
            # Save REAL model state
            checkpoint_data = {
                'epoch': self.current_epoch,
                'step': self.current_step,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'loss': loss,
                'accuracy': accuracy,
                'worker_id': self.worker_id,
                'training_loss_history': self.training_loss,
                'training_accuracy_history': self.training_accuracy,
                'timestamp': datetime.now().isoformat()
            }
            
            # Actually save the checkpoint
            torch.save(checkpoint_data, checkpoint_path)
            
            # Get real file size
            file_size = checkpoint_path.stat().st_size
            
            print(f"üíæ REAL Checkpoint saved: {checkpoint_path.name}")
            print(f"   Size: {file_size / 1024:.1f} KB")
            print(f"   Loss: {loss:.4f}, Accuracy: {accuracy:.2f}%")
            
            # Notify coordinator
            await self.notify_coordinator_checkpoint(str(checkpoint_path), file_size, accuracy, loss)
            
            return checkpoint_path
            
        except Exception as e:
            print(f"‚ùå Checkpoint failed: {e}")
            return None
    
    async def notify_coordinator_checkpoint(self, path: str, size: int, accuracy: float, loss: float):
        """Notify coordinator about new checkpoint"""
        try:
            checkpoint_info = {
                "worker_id": self.worker_id,
                "checkpoint_id": f"real_checkpoint_epoch_{self.current_epoch}_{int(time.time())}",
                "epoch": self.current_epoch,
                "step": self.current_step,
                "path": path,
                "size_bytes": size,
                "accuracy": accuracy,
                "loss": loss,
                "timestamp": datetime.now().isoformat()
            }
            
            # Try to notify coordinator
            try:
                response = requests.post(
                    f"{self.coordinator_url.replace('50052', '51052')}/api/checkpoint",
                    json=checkpoint_info,
                    timeout=1
                )
                print(f"üì§ Checkpoint notification sent to coordinator")
            except:
                print(f"üì§ Checkpoint logged: {checkpoint_info['checkpoint_id']}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Checkpoint notification failed: {e}")
    
    def evaluate_model(self):
        """Evaluate model on test set"""
        self.model.eval()
        correct = 0
        total = 0
        total_loss = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                total_loss += loss.item()
                
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        accuracy = 100. * correct / total
        avg_loss = total_loss / len(self.test_loader)
        
        return accuracy, avg_loss
    
    async def train_epoch(self):
        """Train for one epoch with REAL training"""
        self.model.train()
        epoch_loss = 0
        correct = 0
        total = 0
        
        print(f"üèÉ Training Epoch {self.current_epoch + 1}")
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # Zero gradients
            self.optimizer.zero_grad()
            
            # Forward pass
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # Backward pass
            loss.backward()
            self.optimizer.step()
            
            # Statistics
            epoch_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            self.current_step += 1
            
            # Send heartbeat every 50 steps
            if self.current_step % 50 == 0:
                await self.send_heartbeat()
            
            # Print progress every 100 batches
            if batch_idx % 100 == 0:
                current_loss = loss.item()
                current_acc = 100. * correct / total
                print(f"   Batch {batch_idx:3d}: Loss = {current_loss:.4f}, Acc = {current_acc:.1f}%")
        
        # Epoch completed
        self.current_epoch += 1
        
        # Calculate epoch metrics
        avg_loss = epoch_loss / len(self.train_loader)
        train_accuracy = 100. * correct / total
        
        # Evaluate on test set
        test_accuracy, test_loss = self.evaluate_model()
        
        # Store metrics
        self.training_loss.append(avg_loss)
        self.training_accuracy.append(train_accuracy)
        
        print(f"‚úÖ Epoch {self.current_epoch} Results:")
        print(f"   Train Loss: {avg_loss:.4f}, Train Acc: {train_accuracy:.2f}%")
        print(f"   Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%")
        
        # Create checkpoint
        checkpoint_path = await self.create_real_checkpoint(test_accuracy, test_loss)
        
        return test_accuracy, test_loss
    
    async def start_real_training(self, num_epochs: int = 3):
        """Start REAL distributed training"""
        print("üéØ Starting REAL Distributed Training")
        print("=" * 50)
        print("This will:")
        print("  üìä Generate synthetic dataset")
        print("  üß† Train a real neural network")
        print("  üíæ Create actual model checkpoints")
        print("  üì° Send heartbeats to coordinator")
        print("  üìà Track real training metrics")
        print("=" * 50)
        
        try:
            # Generate dataset
            self.generate_synthetic_dataset()
            
            # Initialize model
            self.initialize_model()
            
            # Training loop
            best_accuracy = 0.0
            for epoch in range(num_epochs):
                accuracy, loss = await self.train_epoch()
                
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    print(f"üéâ New best accuracy: {best_accuracy:.2f}%")
                
                # Simulate barrier synchronization
                print(f"üöß Epoch {epoch + 1} barrier synchronization...")
                await asyncio.sleep(1)
            
            print("üéâ REAL Training Completed!")
            print(f"   Best Test Accuracy: {best_accuracy:.2f}%")
            print(f"   Total Steps: {self.current_step}")
            print(f"   Checkpoints Created: {self.current_epoch}")
            
            # Show checkpoint files
            checkpoint_dir = Path("./checkpoints")
            if checkpoint_dir.exists():
                checkpoints = list(checkpoint_dir.glob(f"*{self.worker_id}*"))
                print(f"   Checkpoint Files: {len(checkpoints)}")
                for cp in checkpoints:
                    size_kb = cp.stat().st_size / 1024
                    print(f"     - {cp.name} ({size_kb:.1f} KB)")
            
        except KeyboardInterrupt:
            print("\nüõë Training interrupted")
        except Exception as e:
            print(f"‚ùå Training failed: {e}")
            raise

async def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Real Distributed Training Demo")
    parser.add_argument("--worker-id", default="real-worker-01", help="Worker ID")
    parser.add_argument("--coordinator", default="http://localhost:50052", help="Coordinator URL")
    parser.add_argument("--epochs", type=int, default=3, help="Number of epochs")
    
    args = parser.parse_args()
    
    # Create and run trainer
    trainer = RealTrainingWorker(args.worker_id, args.coordinator)
    await trainer.start_real_training(args.epochs)

if __name__ == "__main__":
    # Check PyTorch
    try:
        import torch
        print(f"‚úÖ PyTorch {torch.__version__} ready")
    except ImportError:
        print("‚ùå PyTorch required: pip install torch")
        sys.exit(1)
    
    asyncio.run(main())